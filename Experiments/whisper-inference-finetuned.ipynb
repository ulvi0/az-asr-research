{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b145d5f-7763-4b94-af6f-78fddcf46cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from datasets import load_dataset, Audio, Features, Value\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "\n",
    "# Point this to your folder containing model.safetensors, config.json, etc.\n",
    "model_path = \"whisper-finetuned/checkpoint-1611\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd428fbf-d702-4a00-8b94-6386d620b7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078b3daf085f4cd0ad4436198e1a4ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9b0f7e-fc30-4b5f-a7fa-69ff08f9e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e6d4650-035e-4a17-ac2c-1c6b5486ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"az\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0372a2d1-b7ca-4b20-8d2d-37ed2552af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "whisper_asr = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    device=0 if device == \"cuda\" else -1,\n",
    "    chunk_length_s=30,\n",
    "    generate_kwargs={'forced_decoder_ids': forced_decoder_ids}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3fa831d-af49-42ea-8cff-5e34348361fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    \"audio\": Value(\"string\"),            # Force IDs to string\n",
    "    \"audio_filepath\": Value(\"string\"),    # Force audio_path to string (or use Audio(...) if needed)\n",
    "    \"speaker\": Value(\"string\"),\n",
    "    \"transcript\": Value(\"string\")# Force description to string\n",
    "    # Add other fields as needed...\n",
    "})\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"../Data/dataset/combined_dataset.json\",\n",
    "    features = features, \n",
    "    split = 'train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9846d95e-5c2c-4662-aad0-6cb1aab7cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio_filepath\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf1f136-f1ac-45a4-a5c2-c91b34876be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': '1150481435_413589_413590_25232',\n",
       " 'audio_filepath': {'path': '../Data/audio/bcd/voices/1150481435_413589_413590_25232.ogg',\n",
       "  'array': array([1.63709046e-11, 1.45519152e-11, 1.81898940e-11, ...,\n",
       "         1.24341566e-02, 1.14010815e-02, 1.16392896e-02]),\n",
       "  'sampling_rate': 16000},\n",
       " 'speaker': '1150481435',\n",
       " 'transcript': '1233482219165815'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9e8f929-ab9f-4c06-96f6-2b67c61d91e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff20838112f4c7cacd99dcc14820793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11478 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "def transcribe_batch(batch):\n",
    "    # batch[\"audio_path\"] is a list of dicts, each with \"array\" and \"sampling_rate\"\n",
    "    # We'll collect them into a single list of audio inputs\n",
    "    audio_inputs = []\n",
    "    for audio_info in batch[\"audio_filepath\"]:\n",
    "        audio_inputs.append({\n",
    "            \"array\": audio_info[\"array\"],\n",
    "            \"sampling_rate\": audio_info[\"sampling_rate\"]\n",
    "        })\n",
    "    \n",
    "    # Pass the list to the pipeline in one go\n",
    "    results = whisper_asr(audio_inputs, batch_size=16)  \n",
    "    # results is a list of dicts, each with \"text\"\n",
    "\n",
    "    # Return them in a list that matches the input length\n",
    "    transcriptions = [r[\"text\"] for r in results]\n",
    "    return {\"transcription\": transcriptions}\n",
    "\n",
    "# Apply the batched function\n",
    "dataset = dataset.map(transcribe_batch, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c912ec09-849e-45be-8d67-2d5abe128f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'audio_filepath', 'speaker', 'transcript', 'transcription'],\n",
       "    num_rows: 11478\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select_columns(['audi, 'transcript', 'transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c28afe-d6f9-4a25-8cb5-8b795e99c479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
