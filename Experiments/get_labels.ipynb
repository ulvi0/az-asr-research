{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\"bcd\", \"car\", \"dat\", \"id\", \"mon\", \"phn\", \"tim\"]\n",
    "\n",
    "def normalize_transcription(text=\"\", type=\"\"):\n",
    "    result = text\n",
    "    if type in [\"car\", \"id\", \"bcd\", \"phn\"]:\n",
    "        result = re.sub(r'[\\W_]+', '', text).lower()\n",
    "    elif type in [\"dat\", \"mon\", \"tim\"]:\n",
    "        if(isinstance(text, float)):\n",
    "            text = str(text)\n",
    "        result = text.strip().lower()\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_type_and_audio(path = \"\"):\n",
    "    type = path.split(\"/\")[2]\n",
    "    audio = path.split(\"/\")[-1].split('.')[0]\n",
    "    return pd.Series([type, audio])\n",
    "\n",
    "def get_labels(path = \"\", dataset = 'full',types = \"all\",delimiter = \" ~ \"):\n",
    "    if types == \"all\":\n",
    "        types = TYPES.copy()\n",
    "    \n",
    "    if dataset == 'full':\n",
    "        dataset = 'dataset'\n",
    "\n",
    "    dfs = []\n",
    "    for type in types:\n",
    "        df = pd.read_csv(Path(path) / f'{type}_{dataset}.csv', dtype='str')\n",
    "        df['label'] = df['transcript'].apply(normalize_transcription, type=type)\n",
    "        dfs.append(df)\n",
    "    labels = pd.concat(dfs, axis = 0)\n",
    "    return labels\n",
    "\n",
    "def get_inferences(file = \"\", types = \"all\"):\n",
    "    tscp = pd.read_csv(file, dtype='str')\n",
    "    tscp[\"normalized_inference\"] = tscp.apply(lambda x: normalize_transcription(text = x['transcription'],\n",
    "                                             type = x['data_type']), axis = 1)\n",
    "    return tscp\n",
    "\n",
    "def exact_match(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    return float((y_true == y_pred).sum() / len(y_true))\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    # Create a distance matrix with dimensions (m+1) x (n+1)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Initialize the first row and column of the matrix.\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Compute distances for each substring pair.\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,      # deletion\n",
    "                dp[i][j - 1] + 1,      # insertion\n",
    "                dp[i - 1][j - 1] + cost  # substitution\n",
    "            )\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def cer(y_true, y_pred):\n",
    "    total_distance = 0\n",
    "    total_chars = 0\n",
    "\n",
    "    for ref, hyp in zip(y_true, y_pred):\n",
    "        total_distance += levenshtein_distance(ref, hyp)\n",
    "        total_chars += len(ref)\n",
    "\n",
    "    overall_cer = total_distance / total_chars if total_chars > 0 else 0\n",
    "    return overall_cer\n",
    "\n",
    "def wer(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate corpus-level WER by joining all sentences and computing WER on the concatenated texts.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (pd.Series or list of str): Series (or list) of reference transcripts.\n",
    "        y_pred (pd.Series or list of str): Series (or list) of predicted transcripts.\n",
    "        \n",
    "    Returns:\n",
    "        float: Corpus-level Word Error Rate.\n",
    "    \"\"\"\n",
    "\n",
    "    transformation = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(), \n",
    "        jiwer.Strip()\n",
    "    ])\n",
    "\n",
    "    # Convert lists to pandas Series if necessary\n",
    "    if not isinstance(y_true, pd.Series):\n",
    "        y_true = pd.Series(y_true)\n",
    "    if not isinstance(y_pred, pd.Series):\n",
    "        y_pred = pd.Series(y_pred)\n",
    "    \n",
    "    # Join the entire corpus into single strings\n",
    "    ref_corpus = \" \".join(y_true.tolist())\n",
    "    hyp_corpus = \" \".join(y_pred.tolist())\n",
    "    \n",
    "    # Compute the corpus-level WER\n",
    "    return jiwer.wer(ref_corpus, hyp_corpus)\n",
    "\n",
    "\n",
    "def model_benchmark(data_path, model, dataset = \"full\", types = \"all\"):\n",
    "    model_path = Path(data_path) / \"inferences\" / f'{model}.csv'\n",
    "    if dataset == \"full\":\n",
    "        label_path = Path(data_path) / 'dataset' \n",
    "    else:\n",
    "        label_path = Path(data_path) / 'dataset' / dataset\n",
    "\n",
    "    inference = get_inferences(model_path, types = types)\n",
    "\n",
    "    if types == \"all\":\n",
    "        types = TYPES\n",
    "    types_list = [[type] for type in types] + [types]\n",
    "\n",
    "\n",
    "    result = []\n",
    "    for type_item in types_list:\n",
    "        labels = get_labels(label_path, types=type_item, dataset=dataset)\n",
    "        df = inference.merge(labels, on = 'audio') \n",
    "\n",
    "        count = df.shape[0]\n",
    "        EM = exact_match(df['label'], df['normalized_inference'])\n",
    "        CER = cer(df['label'], df['normalized_inference'])\n",
    "        WER = wer(df['label'], df['normalized_inference'])\n",
    "        if len(type_item) == 1:\n",
    "            data_type = type_item[0]\n",
    "        else:\n",
    "            data_type = \"all\"\n",
    "\n",
    "        result.append({\n",
    "            \"data-type\": data_type,\n",
    "            \"count\": count,\n",
    "            \"EM\": EM,\n",
    "            \"CER\": CER,\n",
    "            \"WER\": WER\n",
    "        })\n",
    "    return result\n",
    "\n",
    "def print_asr_benchmark(data, model):\n",
    "    \"\"\"\n",
    "    Prints a formatted ASR model benchmark table.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: list of dictionaries with keys 'data-type', 'count', 'EM', 'CER', and 'WER'\n",
    "    \"\"\"\n",
    "    # Print header\n",
    "    header = \"{:<12} {:<7} {:<8} {:<8} {:<8}\".format(\"Data Type\", \"Count\", \"EM\", \"CER\", \"WER\")\n",
    "    separator = \"-\" * len(header)\n",
    "    \n",
    "    print(f\"{model} benchmark\\n\".upper())\n",
    "    print(header)\n",
    "    print(separator)\n",
    "    \n",
    "    # ANSI escape sequences for bold yellow formatting\n",
    "    BOLD_YELLOW = \"\\033[1;33m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    \n",
    "    # Print each data row; highlight the 'all' row\n",
    "    for record in data:\n",
    "        row = \"{:<12} {:<7} {:.4f}   {:.4f}   {:.4f}\".format(\n",
    "            record[\"data-type\"],\n",
    "            record[\"count\"],\n",
    "            record[\"EM\"],\n",
    "            record[\"CER\"],\n",
    "            record[\"WER\"]\n",
    "        )\n",
    "        if record[\"data-type\"] == \"all\":\n",
    "            row = BOLD_YELLOW + row + RESET\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>audio_filepath_x</th>\n",
       "      <th>data_type</th>\n",
       "      <th>transcription</th>\n",
       "      <th>normalized_inference</th>\n",
       "      <th>audio_filepath_y</th>\n",
       "      <th>speaker</th>\n",
       "      <th>transcript</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>575298922_459913_459922_25170</td>\n",
       "      <td>/train/bank-cards/voices/575298922_459913_4599...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>29-74, 51-98, 17-81, 61-72</td>\n",
       "      <td>2974519817816172</td>\n",
       "      <td>../Data/audio/bcd/voices/575298922_459913_4599...</td>\n",
       "      <td>575298922</td>\n",
       "      <td>2974519817816172</td>\n",
       "      <td>2974519817816172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1229817727_454607_454617_25403</td>\n",
       "      <td>/train/bank-cards/voices/1229817727_454607_454...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>17-88-23-12-41-59-89-61</td>\n",
       "      <td>1788231241598961</td>\n",
       "      <td>../Data/audio/bcd/voices/1229817727_454607_454...</td>\n",
       "      <td>1229817727</td>\n",
       "      <td>1788231241598961</td>\n",
       "      <td>1788231241598961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>828296631_470091_470123_25362</td>\n",
       "      <td>/train/bank-cards/voices/828296631_470091_4701...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>12-31-99-93-33-99-37-82</td>\n",
       "      <td>1231999333993782</td>\n",
       "      <td>../Data/audio/bcd/voices/828296631_470091_4701...</td>\n",
       "      <td>828296631</td>\n",
       "      <td>1231999333993782</td>\n",
       "      <td>1231999333993782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>314849420_422111_422114_25221</td>\n",
       "      <td>/train/bank-cards/voices/314849420_422111_4221...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>22-48-7369-22-684-447</td>\n",
       "      <td>2248736922684447</td>\n",
       "      <td>../Data/audio/bcd/voices/314849420_422111_4221...</td>\n",
       "      <td>314849420</td>\n",
       "      <td>2248736922684447</td>\n",
       "      <td>2248736922684447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>341881303_439547_439556_25304</td>\n",
       "      <td>/train/bank-cards/voices/341881303_439547_4395...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>2, 6, 4, 7, 3, 9, 4, 6, 2, 8, 4, 9, 8, 1, 5, 8</td>\n",
       "      <td>2647394628498158</td>\n",
       "      <td>../Data/audio/bcd/voices/341881303_439547_4395...</td>\n",
       "      <td>341881303</td>\n",
       "      <td>2647394628498158</td>\n",
       "      <td>2647394628498158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>314849420_476149_476188_25173</td>\n",
       "      <td>/train/bank-cards/voices/314849420_476149_4761...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>58, 24, 16, 13, 71, 51, 76, 63</td>\n",
       "      <td>5824161371517663</td>\n",
       "      <td>../Data/audio/bcd/voices/314849420_476149_4761...</td>\n",
       "      <td>314849420</td>\n",
       "      <td>5824161371517663</td>\n",
       "      <td>5824161371517663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1347919237_445797_445869_25324</td>\n",
       "      <td>/train/bank-cards/voices/1347919237_445797_445...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>14, 82, 69, 87, 28, 18, 15, 15</td>\n",
       "      <td>1482698728181515</td>\n",
       "      <td>../Data/audio/bcd/voices/1347919237_445797_445...</td>\n",
       "      <td>1347919237</td>\n",
       "      <td>1482698728181515</td>\n",
       "      <td>1482698728181515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>1297961140_475537_475548_25883</td>\n",
       "      <td>/train/bank-cards/voices/1297961140_475537_475...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>1, 4, 9, 4, 1, 1, 3, 4, 7, 5, 8, 7, 8, 6, 3, 9</td>\n",
       "      <td>1494113475878639</td>\n",
       "      <td>../Data/audio/bcd/voices/1297961140_475537_475...</td>\n",
       "      <td>1297961140</td>\n",
       "      <td>1494113475878639</td>\n",
       "      <td>1494113475878639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>619589413_437632_437637_25483</td>\n",
       "      <td>/train/bank-cards/voices/619589413_437632_4376...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>24-65-95-94-64-29-14-82</td>\n",
       "      <td>2465959464291482</td>\n",
       "      <td>../Data/audio/bcd/voices/619589413_437632_4376...</td>\n",
       "      <td>619589413</td>\n",
       "      <td>2465959464291482</td>\n",
       "      <td>2465959464291482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>1043664892_430517_430518_25356</td>\n",
       "      <td>/train/bank-cards/voices/1043664892_430517_430...</td>\n",
       "      <td>bcd</td>\n",
       "      <td>64, 76, 94, 12, 55, 38, 58, 64</td>\n",
       "      <td>6476941255385864</td>\n",
       "      <td>../Data/audio/bcd/voices/1043664892_430517_430...</td>\n",
       "      <td>1043664892</td>\n",
       "      <td>6476941255385864</td>\n",
       "      <td>6476941255385864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>755 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              audio  \\\n",
       "0     575298922_459913_459922_25170   \n",
       "1    1229817727_454607_454617_25403   \n",
       "2     828296631_470091_470123_25362   \n",
       "3     314849420_422111_422114_25221   \n",
       "4     341881303_439547_439556_25304   \n",
       "..                              ...   \n",
       "750   314849420_476149_476188_25173   \n",
       "751  1347919237_445797_445869_25324   \n",
       "752  1297961140_475537_475548_25883   \n",
       "753   619589413_437632_437637_25483   \n",
       "754  1043664892_430517_430518_25356   \n",
       "\n",
       "                                      audio_filepath_x data_type  \\\n",
       "0    /train/bank-cards/voices/575298922_459913_4599...       bcd   \n",
       "1    /train/bank-cards/voices/1229817727_454607_454...       bcd   \n",
       "2    /train/bank-cards/voices/828296631_470091_4701...       bcd   \n",
       "3    /train/bank-cards/voices/314849420_422111_4221...       bcd   \n",
       "4    /train/bank-cards/voices/341881303_439547_4395...       bcd   \n",
       "..                                                 ...       ...   \n",
       "750  /train/bank-cards/voices/314849420_476149_4761...       bcd   \n",
       "751  /train/bank-cards/voices/1347919237_445797_445...       bcd   \n",
       "752  /train/bank-cards/voices/1297961140_475537_475...       bcd   \n",
       "753  /train/bank-cards/voices/619589413_437632_4376...       bcd   \n",
       "754  /train/bank-cards/voices/1043664892_430517_430...       bcd   \n",
       "\n",
       "                                      transcription normalized_inference  \\\n",
       "0                        29-74, 51-98, 17-81, 61-72     2974519817816172   \n",
       "1                           17-88-23-12-41-59-89-61     1788231241598961   \n",
       "2                           12-31-99-93-33-99-37-82     1231999333993782   \n",
       "3                             22-48-7369-22-684-447     2248736922684447   \n",
       "4    2, 6, 4, 7, 3, 9, 4, 6, 2, 8, 4, 9, 8, 1, 5, 8     2647394628498158   \n",
       "..                                              ...                  ...   \n",
       "750                  58, 24, 16, 13, 71, 51, 76, 63     5824161371517663   \n",
       "751                  14, 82, 69, 87, 28, 18, 15, 15     1482698728181515   \n",
       "752  1, 4, 9, 4, 1, 1, 3, 4, 7, 5, 8, 7, 8, 6, 3, 9     1494113475878639   \n",
       "753                         24-65-95-94-64-29-14-82     2465959464291482   \n",
       "754                  64, 76, 94, 12, 55, 38, 58, 64     6476941255385864   \n",
       "\n",
       "                                      audio_filepath_y     speaker  \\\n",
       "0    ../Data/audio/bcd/voices/575298922_459913_4599...   575298922   \n",
       "1    ../Data/audio/bcd/voices/1229817727_454607_454...  1229817727   \n",
       "2    ../Data/audio/bcd/voices/828296631_470091_4701...   828296631   \n",
       "3    ../Data/audio/bcd/voices/314849420_422111_4221...   314849420   \n",
       "4    ../Data/audio/bcd/voices/341881303_439547_4395...   341881303   \n",
       "..                                                 ...         ...   \n",
       "750  ../Data/audio/bcd/voices/314849420_476149_4761...   314849420   \n",
       "751  ../Data/audio/bcd/voices/1347919237_445797_445...  1347919237   \n",
       "752  ../Data/audio/bcd/voices/1297961140_475537_475...  1297961140   \n",
       "753  ../Data/audio/bcd/voices/619589413_437632_4376...   619589413   \n",
       "754  ../Data/audio/bcd/voices/1043664892_430517_430...  1043664892   \n",
       "\n",
       "           transcript             label  \n",
       "0    2974519817816172  2974519817816172  \n",
       "1    1788231241598961  1788231241598961  \n",
       "2    1231999333993782  1231999333993782  \n",
       "3    2248736922684447  2248736922684447  \n",
       "4    2647394628498158  2647394628498158  \n",
       "..                ...               ...  \n",
       "750  5824161371517663  5824161371517663  \n",
       "751  1482698728181515  1482698728181515  \n",
       "752  1494113475878639  1494113475878639  \n",
       "753  2465959464291482  2465959464291482  \n",
       "754  6476941255385864  6476941255385864  \n",
       "\n",
       "[755 rows x 9 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../Data'\n",
    "model = 'whisper-large-v3'\n",
    "dataset = 'train'\n",
    "types = ['bcd']\n",
    "model_path = Path(data_path) / \"inferences\" / f'{model}.csv'\n",
    "label_path = Path(data_path) / 'dataset' \n",
    "\n",
    "inference = get_inferences(model_path, types = types)\n",
    "labels = get_labels(label_path, types=types)\n",
    "df = inference.merge(labels, on = 'audio') \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHISPER-LARGE-V3 BENCHMARK\n",
      "\n",
      "Data Type    Count   EM       CER      WER     \n",
      "-----------------------------------------------\n",
      "bcd          111     0.8559   0.0152   0.1441\n",
      "car          122     0.5328   0.1756   0.4672\n",
      "dat          655     0.1435   0.4835   0.7763\n",
      "id           391     0.4041   0.3073   0.5959\n",
      "mon          503     0.1193   0.3262   0.5317\n",
      "phn          125     0.8480   0.0504   0.1520\n",
      "tim          27      0.0000   0.3780   0.6778\n",
      "\u001b[1;33mall          1934    0.2989   0.3589   0.6218\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "types = TYPES\n",
    "model = \"whisper-large-v3\"\n",
    "result = model_benchmark(\"../Data\", model, dataset='test', types = types)\n",
    "print_asr_benchmark(result, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHISPER-TURBO BENCHMARK\n",
      "\n",
      "Data Type    Count   EM       CER      WER     \n",
      "-----------------------------------------------\n",
      "bcd          111     0.6847   1.0890   0.3153\n",
      "car          122     0.2869   1.8829   0.7131\n",
      "dat          655     0.0107   1.0091   1.1483\n",
      "id           391     0.2890   1.2889   0.7110\n",
      "mon          503     0.1352   0.7298   0.6943\n",
      "phn          125     0.5120   1.4768   0.4880\n",
      "tim          27      0.0370   0.9319   1.1556\n",
      "\u001b[1;33mall          1934    0.1882   1.0058   0.8795\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "types = TYPES\n",
    "model = \"whisper-turbo\"\n",
    "result = model_benchmark(\"../Data\", model, dataset='test',types = types)\n",
    "print_asr_benchmark(result, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHISPER-LARGE-V3 BENCHMARK\n",
      "\n",
      "Data Type    Count   EM       CER      WER     \n",
      "-----------------------------------------------\n",
      "phn          789     0.8365   0.0736   0.1635\n",
      "bcd          755     0.8596   0.0864   0.1404\n",
      "\u001b[1;33mall          1544    0.8478   0.0814   0.1522\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "types = ['phn', 'bcd']\n",
    "model = \"whisper-large-v3\"\n",
    "result = model_benchmark(\"../Data\", model, types = types)\n",
    "print_asr_benchmark(result, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHISPER-LARGE-V3 BENCHMARK\n",
      "\n",
      "Data Type    Count   EM       CER      WER     \n",
      "-----------------------------------------------\n",
      "id           2343    0.4106   0.2786   0.5894\n",
      "car          693     0.4935   0.2482   0.5065\n",
      "\u001b[1;33mall          3036    0.4295   0.2730   0.5705\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "types = ['id', 'car']\n",
    "model = \"whisper-large-v3\"\n",
    "result = model_benchmark(\"../Data\", model, types = types)\n",
    "print_asr_benchmark(result, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHISPER-LARGE-V3 BENCHMARK\n",
      "\n",
      "Data Type    Count   EM       CER      WER     \n",
      "-----------------------------------------------\n",
      "dat          4079    0.1701   0.4179   0.6910\n",
      "tim          169     0.0296   0.3203   0.6003\n",
      "mon          2650    0.0962   0.3685   0.5673\n",
      "\u001b[1;33mall          6898    0.1383   0.3978   0.6332\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "types = ['dat', 'tim', 'mon']\n",
    "model = \"whisper-large-v3\"\n",
    "result = model_benchmark(\"../Data\", model, types = types)\n",
    "print_asr_benchmark(result, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
