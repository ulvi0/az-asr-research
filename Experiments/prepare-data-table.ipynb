{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data type: bcd\n",
      "  Found 755 valid entries for bcd\n",
      "  Saved CSV : ../Data/dataset/bcd_dataset.csv\n",
      "  Saved JSON: ../Data/dataset/bcd_dataset.json\n",
      "Processing data type: car\n",
      "  Found 693 valid entries for car\n",
      "  Saved CSV : ../Data/dataset/car_dataset.csv\n",
      "  Saved JSON: ../Data/dataset/car_dataset.json\n",
      "Processing data type: dat\n",
      "  Found 4079 valid entries for dat\n",
      "  Saved CSV : ../Data/dataset/dat_dataset.csv\n",
      "  Saved JSON: ../Data/dataset/dat_dataset.json\n",
      "Processing data type: id\n",
      "  Found 2343 valid entries for id\n",
      "  Saved CSV : ../Data/dataset/id_dataset.csv\n",
      "  Saved JSON: ../Data/dataset/id_dataset.json\n",
      "Processing data type: mon\n",
      "  Found 2650 valid entries for mon\n",
      "  Saved CSV : ../Data/dataset/mon_dataset.csv\n",
      "  Saved JSON: ../Data/dataset/mon_dataset.json\n",
      "Processing data type: phn\n",
      "  Found 789 valid entries for phn\n",
      "  Saved CSV : ../Data/dataset/phn_dataset.csv\n",
      "  Saved JSON: ../Data/dataset/phn_dataset.json\n",
      "Processing data type: tim\n",
      "  Found 169 valid entries for tim\n",
      "  Saved CSV : ../Data/dataset/tim_dataset.csv\n",
      "  Saved JSON: ../Data/dataset/tim_dataset.json\n",
      "\n",
      "Created combined CSV : ../Data/dataset/combined_dataset.csv\n",
      "Created combined JSON: ../Data/dataset/combined_dataset.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# --------------------------\n",
    "# 1. Configuration\n",
    "# --------------------------\n",
    "BASE_DATA_PATH = \"../Data\"       # Adjust if the notebook is in Experiments\n",
    "LABELS_PATH    = os.path.join(BASE_DATA_PATH, \"labels\")\n",
    "AUDIO_PATH     = os.path.join(BASE_DATA_PATH, \"audio\")\n",
    "\n",
    "# Where to save final CSV/JSON. We'll create a new folder \"dataset\" inside Data.\n",
    "OUTPUT_DATASET_PATH = os.path.join(BASE_DATA_PATH, \"dataset\")\n",
    "os.makedirs(OUTPUT_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "# Example list of data types (3-letter folder names).\n",
    "DATA_TYPES = [\"bcd\", \"car\", \"dat\", \"id\", \"mon\", \"phn\", \"tim\"]\n",
    "\n",
    "# --------------------------\n",
    "# 2. Helper Functions\n",
    "# --------------------------\n",
    "def build_dataset_for_type(data_type, label_file_suffix=\"txt\", audio_ext=\"ogg\"):\n",
    "    \"\"\"\n",
    "    Reads a label file (e.g., bcd.txt) in LABELS_PATH,\n",
    "    filters entries to those that have a matching .ogg in Data/audio/<data_type>/voices/,\n",
    "    extracts speaker from the first underscore,\n",
    "    and returns a list of dicts: {\n",
    "        \"audio\":          <file name without extension>,\n",
    "        \"audio_filepath\": <full path to .ogg>,\n",
    "        \"speaker\":        <speaker ID>,\n",
    "        \"transcript\":     <transcript string>\n",
    "    }.\n",
    "    \"\"\"\n",
    "    label_file = os.path.join(LABELS_PATH, f\"{data_type}.{label_file_suffix}\")\n",
    "    audio_folder = os.path.join(AUDIO_PATH, data_type, \"voices\")\n",
    "\n",
    "    if not os.path.isfile(label_file):\n",
    "        print(f\"[Warning] Label file not found: {label_file}\")\n",
    "        return []\n",
    "\n",
    "    if not os.path.isdir(audio_folder):\n",
    "        print(f\"[Warning] Audio folder not found: {audio_folder}\")\n",
    "        return []\n",
    "\n",
    "    dataset = []\n",
    "    with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Expecting something like: \"filename ~ transcript\"\n",
    "            parts = line.split(\"~\")\n",
    "            if len(parts) != 2:\n",
    "                continue  # skip malformed lines\n",
    "\n",
    "            audio_name = parts[0].strip()    # e.g. \"1150481435_413681_413682_...\"\n",
    "            transcript = parts[1].strip()\n",
    "\n",
    "            # Construct full path to .ogg\n",
    "            audio_file = os.path.join(audio_folder, audio_name + f\".{audio_ext}\")\n",
    "\n",
    "            # Check if the audio file exists\n",
    "            if os.path.isfile(audio_file):\n",
    "                # Speaker is everything before the first underscore\n",
    "                speaker = audio_name.split(\"_\")[0]\n",
    "\n",
    "                dataset.append({\n",
    "                    \"audio\": audio_name,\n",
    "                    \"audio_filepath\": audio_file,\n",
    "                    \"speaker\": speaker,\n",
    "                    \"transcript\": transcript\n",
    "                })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_to_csv(data_list, output_csv):\n",
    "    \"\"\"\n",
    "    Saves a list of dicts to CSV with columns:\n",
    "    [audio, audio_filepath, speaker, transcript].\n",
    "    \"\"\"\n",
    "    fieldnames = [\"audio\", \"audio_filepath\", \"speaker\", \"transcript\"]\n",
    "    with open(output_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def save_to_json(data_list, output_json):\n",
    "    \"\"\"\n",
    "    Saves a list of dicts to JSON (with the same columns).\n",
    "    \"\"\"\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Main Process\n",
    "# --------------------------\n",
    "all_data = []\n",
    "\n",
    "for dt in DATA_TYPES:\n",
    "    print(f\"Processing data type: {dt}\")\n",
    "    dataset = build_dataset_for_type(dt)\n",
    "    print(f\"  Found {len(dataset)} valid entries for {dt}\")\n",
    "\n",
    "    if dataset:\n",
    "        csv_path = os.path.join(OUTPUT_DATASET_PATH, f\"{dt}_dataset.csv\")\n",
    "        json_path = os.path.join(OUTPUT_DATASET_PATH, f\"{dt}_dataset.json\")\n",
    "\n",
    "        save_to_csv(dataset, csv_path)\n",
    "        save_to_json(dataset, json_path)\n",
    "\n",
    "        print(f\"  Saved CSV : {csv_path}\")\n",
    "        print(f\"  Saved JSON: {json_path}\")\n",
    "\n",
    "    # Optionally add to a global list for one big combined file\n",
    "    all_data.extend(dataset)\n",
    "\n",
    "# If you want a single combined CSV/JSON for everything:\n",
    "if all_data:\n",
    "    combined_csv = os.path.join(OUTPUT_DATASET_PATH, \"combined_dataset.csv\")\n",
    "    combined_json = os.path.join(OUTPUT_DATASET_PATH, \"combined_dataset.json\")\n",
    "\n",
    "    save_to_csv(all_data, combined_csv)\n",
    "    save_to_json(all_data, combined_json)\n",
    "\n",
    "    print(\"\\nCreated combined CSV :\", combined_csv)\n",
    "    print(\"Created combined JSON:\", combined_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
